*labels*:
Is the thing we're predicting, the y in the (x,y) variable in simple linear regression. the label could be the future price of
something or just any random thing

*Features*: 
 A feature is an input variable that we're using to train our y variable.
a simple ML project could use a single feature to millions or billions of features specified as
x1,x2,x3,x4,.....,xn

*labeled examples*:
A labeled example includes both label and feature(x)  {features, label}: (x, y)

*unlabeled examples*:
an unlabeled example contains features but not the label {features, ?}: (x, ?)


Use labeled examples to train the model. 
In our spam detector example, the labeled examples would be individual emails that users have explicitly marked as "spam" or "not spam."
Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples

Models:
A model defined the relationship between a label and a feature.
Highlight of a model's life

*Training*:
Training means creating or learning the model. We do this by showing the model labeled examples (that both feature and labels)
to enable the model gradually learn the relationship betweenTHE Features and the label.

*Inference*:
Inference means applying the trained model to unlabeled examples.
That is, us the trained model to make predictions (y).


Linear regression: 
In statistics, linear regression is a statistical model which estimates the linear relationship between a scalar 
response and one or more explanatory variables (also known as dependent and independent variables). 

 A linear regression line has an equation of Y = a + bX, where X is the explanatory variable and Y is the dependent variable. 
 If there appears to be no association between the proposed explanatory and dependent variables, 
 fitting a linear regression model to the data may not provide a useful model.


in machine learning, our equation for linear regression is given as y=wx+b

y is the predicted label (a desired output).
b  is the predicted label (a desired output).
w is the weight of feature 1. Weight is the same concept as the "slope" m
 in the traditional equation of a line.
x  is a feature (a known input).

the equation above is for a model with just one feature, when we're working on more sophisticated Models
where we need many features our equation becomes
y= w1x1+ w2x2+w3x3+...+wnxn



Training and Loss:

Training a model means learning (determining) good values for all the weights and the bias from labeled examples.
In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting 
to find a model that minimizes loss; this process is called empirical risk minimization.


Loss:
loss is a number indicating how bad the model's prediction was on a single example. 
If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. 
The goal of training a model is to find a set of weights and biases that have low loss, 
on average, across all examples.

Squared loss (L2 loss):
  = the square of the difference between the label and the prediction
  = (observation - prediction(x))2
  = (y - y')2

  Mean Squared Error (MSE):
  Is the average squared loss per example over the whole data set.
  E(y-y')/N
